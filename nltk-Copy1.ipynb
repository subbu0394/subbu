{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gutenberg', 'gutenberg.zip', 'stopwords', 'stopwords.zip', 'wordnet', 'wordnet.zip', 'words', 'words.zip']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(nltk.data.find(\"corpora\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\naruk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\naruk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('movie_reviews')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamlet=nltk.corpus.gutenberg.words(\"shakespeare-hamlet.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[', 'The', 'Tragedie', 'of', 'Hamlet', 'by', ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamlet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ The Tragedie of Hamlet by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Barnardo and Francisco two Centinels . Barnardo . Who ' s there ? Fran . Nay answer me : Stand & vnfold your selfe Bar . Long liue the King Fran . Barnardo "
     ]
    }
   ],
   "source": [
    "for word in hamlet[:50]:\n",
    "    print(word,sep=' ',end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai=\"Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using rules to reach approximate or definite conclusions) and self-correction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_tokens=word_tokenize(ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " '(',\n",
       " 'AI',\n",
       " ')',\n",
       " 'is',\n",
       " 'the',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " 'processes',\n",
       " 'by',\n",
       " 'machines',\n",
       " ',',\n",
       " 'especially',\n",
       " 'computer',\n",
       " 'systems',\n",
       " '.',\n",
       " 'These',\n",
       " 'processes',\n",
       " 'include',\n",
       " 'learning',\n",
       " '(',\n",
       " 'the',\n",
       " 'acquisition',\n",
       " 'of',\n",
       " 'information',\n",
       " 'and',\n",
       " 'rules',\n",
       " 'for',\n",
       " 'using',\n",
       " 'the',\n",
       " 'information',\n",
       " ')',\n",
       " ',',\n",
       " 'reasoning',\n",
       " '(',\n",
       " 'using',\n",
       " 'rules',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'approximate',\n",
       " 'or',\n",
       " 'definite',\n",
       " 'conclusions',\n",
       " ')',\n",
       " 'and',\n",
       " 'self-correction',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ai_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "fdist=FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'(': 3, ')': 3, 'the': 3, 'intelligence': 2, 'of': 2, 'processes': 2, ',': 2, '.': 2, 'information': 2, 'and': 2, ...})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in ai_tokens:\n",
    "    fdist[word.lower()]+=1\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('(', 3), (')', 3), ('the', 3), ('intelligence', 2), ('of', 2), ('processes', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(fdist.most_common(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import blankline_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_blank=blankline_tokenize(ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial intelligence (AI) is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning (the acquisition of information and rules for using the information), reasoning (using rules to reach approximate or definite conclusions) and self-correction.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ai_blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading util: Package 'util' not found in index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"util\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams,trigrams,ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_bigram=list(nltk.bigrams(ai_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artificial', 'intelligence'),\n",
       " ('intelligence', '('),\n",
       " ('(', 'AI'),\n",
       " ('AI', ')'),\n",
       " (')', 'is'),\n",
       " ('is', 'the'),\n",
       " ('the', 'simulation'),\n",
       " ('simulation', 'of'),\n",
       " ('of', 'human'),\n",
       " ('human', 'intelligence'),\n",
       " ('intelligence', 'processes'),\n",
       " ('processes', 'by'),\n",
       " ('by', 'machines'),\n",
       " ('machines', ','),\n",
       " (',', 'especially'),\n",
       " ('especially', 'computer'),\n",
       " ('computer', 'systems'),\n",
       " ('systems', '.'),\n",
       " ('.', 'These'),\n",
       " ('These', 'processes'),\n",
       " ('processes', 'include'),\n",
       " ('include', 'learning'),\n",
       " ('learning', '('),\n",
       " ('(', 'the'),\n",
       " ('the', 'acquisition'),\n",
       " ('acquisition', 'of'),\n",
       " ('of', 'information'),\n",
       " ('information', 'and'),\n",
       " ('and', 'rules'),\n",
       " ('rules', 'for'),\n",
       " ('for', 'using'),\n",
       " ('using', 'the'),\n",
       " ('the', 'information'),\n",
       " ('information', ')'),\n",
       " (')', ','),\n",
       " (',', 'reasoning'),\n",
       " ('reasoning', '('),\n",
       " ('(', 'using'),\n",
       " ('using', 'rules'),\n",
       " ('rules', 'to'),\n",
       " ('to', 'reach'),\n",
       " ('reach', 'approximate'),\n",
       " ('approximate', 'or'),\n",
       " ('or', 'definite'),\n",
       " ('definite', 'conclusions'),\n",
       " ('conclusions', ')'),\n",
       " (')', 'and'),\n",
       " ('and', 'self-correction'),\n",
       " ('self-correction', '.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_trigram=list(nltk.trigrams(ai_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artificial', 'intelligence', '('),\n",
       " ('intelligence', '(', 'AI'),\n",
       " ('(', 'AI', ')'),\n",
       " ('AI', ')', 'is'),\n",
       " (')', 'is', 'the'),\n",
       " ('is', 'the', 'simulation'),\n",
       " ('the', 'simulation', 'of'),\n",
       " ('simulation', 'of', 'human'),\n",
       " ('of', 'human', 'intelligence'),\n",
       " ('human', 'intelligence', 'processes'),\n",
       " ('intelligence', 'processes', 'by'),\n",
       " ('processes', 'by', 'machines'),\n",
       " ('by', 'machines', ','),\n",
       " ('machines', ',', 'especially'),\n",
       " (',', 'especially', 'computer'),\n",
       " ('especially', 'computer', 'systems'),\n",
       " ('computer', 'systems', '.'),\n",
       " ('systems', '.', 'These'),\n",
       " ('.', 'These', 'processes'),\n",
       " ('These', 'processes', 'include'),\n",
       " ('processes', 'include', 'learning'),\n",
       " ('include', 'learning', '('),\n",
       " ('learning', '(', 'the'),\n",
       " ('(', 'the', 'acquisition'),\n",
       " ('the', 'acquisition', 'of'),\n",
       " ('acquisition', 'of', 'information'),\n",
       " ('of', 'information', 'and'),\n",
       " ('information', 'and', 'rules'),\n",
       " ('and', 'rules', 'for'),\n",
       " ('rules', 'for', 'using'),\n",
       " ('for', 'using', 'the'),\n",
       " ('using', 'the', 'information'),\n",
       " ('the', 'information', ')'),\n",
       " ('information', ')', ','),\n",
       " (')', ',', 'reasoning'),\n",
       " (',', 'reasoning', '('),\n",
       " ('reasoning', '(', 'using'),\n",
       " ('(', 'using', 'rules'),\n",
       " ('using', 'rules', 'to'),\n",
       " ('rules', 'to', 'reach'),\n",
       " ('to', 'reach', 'approximate'),\n",
       " ('reach', 'approximate', 'or'),\n",
       " ('approximate', 'or', 'definite'),\n",
       " ('or', 'definite', 'conclusions'),\n",
       " ('definite', 'conclusions', ')'),\n",
       " ('conclusions', ')', 'and'),\n",
       " (')', 'and', 'self-correction'),\n",
       " ('and', 'self-correction', '.')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_ngrams=list(nltk.ngrams(ai_tokens,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artificial', 'intelligence', '(', 'AI', ')'),\n",
       " ('intelligence', '(', 'AI', ')', 'is'),\n",
       " ('(', 'AI', ')', 'is', 'the'),\n",
       " ('AI', ')', 'is', 'the', 'simulation'),\n",
       " (')', 'is', 'the', 'simulation', 'of'),\n",
       " ('is', 'the', 'simulation', 'of', 'human'),\n",
       " ('the', 'simulation', 'of', 'human', 'intelligence'),\n",
       " ('simulation', 'of', 'human', 'intelligence', 'processes'),\n",
       " ('of', 'human', 'intelligence', 'processes', 'by'),\n",
       " ('human', 'intelligence', 'processes', 'by', 'machines'),\n",
       " ('intelligence', 'processes', 'by', 'machines', ','),\n",
       " ('processes', 'by', 'machines', ',', 'especially'),\n",
       " ('by', 'machines', ',', 'especially', 'computer'),\n",
       " ('machines', ',', 'especially', 'computer', 'systems'),\n",
       " (',', 'especially', 'computer', 'systems', '.'),\n",
       " ('especially', 'computer', 'systems', '.', 'These'),\n",
       " ('computer', 'systems', '.', 'These', 'processes'),\n",
       " ('systems', '.', 'These', 'processes', 'include'),\n",
       " ('.', 'These', 'processes', 'include', 'learning'),\n",
       " ('These', 'processes', 'include', 'learning', '('),\n",
       " ('processes', 'include', 'learning', '(', 'the'),\n",
       " ('include', 'learning', '(', 'the', 'acquisition'),\n",
       " ('learning', '(', 'the', 'acquisition', 'of'),\n",
       " ('(', 'the', 'acquisition', 'of', 'information'),\n",
       " ('the', 'acquisition', 'of', 'information', 'and'),\n",
       " ('acquisition', 'of', 'information', 'and', 'rules'),\n",
       " ('of', 'information', 'and', 'rules', 'for'),\n",
       " ('information', 'and', 'rules', 'for', 'using'),\n",
       " ('and', 'rules', 'for', 'using', 'the'),\n",
       " ('rules', 'for', 'using', 'the', 'information'),\n",
       " ('for', 'using', 'the', 'information', ')'),\n",
       " ('using', 'the', 'information', ')', ','),\n",
       " ('the', 'information', ')', ',', 'reasoning'),\n",
       " ('information', ')', ',', 'reasoning', '('),\n",
       " (')', ',', 'reasoning', '(', 'using'),\n",
       " (',', 'reasoning', '(', 'using', 'rules'),\n",
       " ('reasoning', '(', 'using', 'rules', 'to'),\n",
       " ('(', 'using', 'rules', 'to', 'reach'),\n",
       " ('using', 'rules', 'to', 'reach', 'approximate'),\n",
       " ('rules', 'to', 'reach', 'approximate', 'or'),\n",
       " ('to', 'reach', 'approximate', 'or', 'definite'),\n",
       " ('reach', 'approximate', 'or', 'definite', 'conclusions'),\n",
       " ('approximate', 'or', 'definite', 'conclusions', ')'),\n",
       " ('or', 'definite', 'conclusions', ')', 'and'),\n",
       " ('definite', 'conclusions', ')', 'and', 'self-correction'),\n",
       " ('conclusions', ')', 'and', 'self-correction', '.')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pst=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'avail'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"availability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[\"give\",\"gave\",\"given\",\"giving\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "gave:gave\n",
      "given:given\n",
      "giving:give\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\":\"+pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:giv\n",
      "gave:gav\n",
      "given:giv\n",
      "giving:giv\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\":\"+lst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lem=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_lem.lemmatize(\"corpora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give:give\n",
      "gave:gave\n",
      "given:given\n",
      "giving:giving\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+\":\"+word_lem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_t=fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation=re.compile(r\"[-.?,!:;()|0-9]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "po_pun=[]\n",
    "for words in ai_tokens:\n",
    "    word=punctuation.sub(\"\",words)\n",
    "    if len(word)>0:\n",
    "        po_pun.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intelligence',\n",
       " 'AI',\n",
       " 'is',\n",
       " 'the',\n",
       " 'simulation',\n",
       " 'of',\n",
       " 'human',\n",
       " 'intelligence',\n",
       " 'processes',\n",
       " 'by',\n",
       " 'machines',\n",
       " 'especially',\n",
       " 'computer',\n",
       " 'systems',\n",
       " 'These',\n",
       " 'processes',\n",
       " 'include',\n",
       " 'learning',\n",
       " 'the',\n",
       " 'acquisition',\n",
       " 'of',\n",
       " 'information',\n",
       " 'and',\n",
       " 'rules',\n",
       " 'for',\n",
       " 'using',\n",
       " 'the',\n",
       " 'information',\n",
       " 'reasoning',\n",
       " 'using',\n",
       " 'rules',\n",
       " 'to',\n",
       " 'reach',\n",
       " 'approximate',\n",
       " 'or',\n",
       " 'definite',\n",
       " 'conclusions',\n",
       " 'and',\n",
       " 'selfcorrection']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "po_pun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(po_pun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent=\"timothy is a natural when it comes to drawing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_tokens=word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('timothy', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('natural', 'JJ')]\n",
      "[('when', 'WRB')]\n",
      "[('it', 'PRP')]\n",
      "[('comes', 'VBZ')]\n",
      "[('to', 'TO')]\n",
      "[('drawing', 'VBG')]\n"
     ]
    }
   ],
   "source": [
    "for token in sent_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('veerendra', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('good', 'JJ')]\n",
      "[('boy', 'NN')]\n",
      "[('and', 'CC')]\n",
      "[('ammu', 'NN')]\n",
      "[('is', 'VBZ')]\n",
      "[('a', 'DT')]\n",
      "[('good', 'JJ')]\n",
      "[('girl', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sent2=\"veerendra is good  boy and ammu is a good girl\"\n",
    "sent_tokens2=word_tokenize(sent2)\n",
    "for token in sent_tokens2:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_sent=\"The US President stays in WHITE HOUSE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tokens=word_tokenize(ne_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "ne_tags=nltk.pos_tag(ne_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  The/DT\n",
      "  (ORGANIZATION US/NNP)\n",
      "  President/NNP\n",
      "  stays/VBZ\n",
      "  in/IN\n",
      "  (FACILITY WHITE/NNP HOUSE/NNP))\n"
     ]
    }
   ],
   "source": [
    "ne_ner=ne_chunk(ne_tags)\n",
    "print(ne_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('cat', 'NN'),\n",
       " ('ate', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('mouse', 'NN'),\n",
       " ('who', 'WP'),\n",
       " ('was', 'VBD'),\n",
       " ('after', 'IN'),\n",
       " ('fresh', 'JJ'),\n",
       " ('cheese', 'NN')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new=\"The big cat ate the little mouse who was after fresh cheese\"\n",
    "new_tokens=nltk.pos_tag(word_tokenize(new))\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer_np=r\"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser=nltk.RegexpParser(grammer_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_result=chunk_parser.parse(new_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m                             \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PATH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m                         )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    696\u001b[0m         find_binary_iter(\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         )\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    680\u001b[0m     for file in find_file_iter(\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m     ):\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m                                         \"https://docs.brew.sh/Installation then `brew install ghostscript`\")                \n\u001b[0;32m    818\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('NP', [('The', 'DT'), ('big', 'JJ'), ('cat', 'NN')]), ('ate', 'VBD'), Tree('NP', [('the', 'DT'), ('little', 'JJ'), ('mouse', 'NN')]), ('who', 'WP'), ('was', 'VBD'), ('after', 'IN'), Tree('NP', [('fresh', 'JJ'), ('cheese', 'NN')])])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neg', 'pos']\n"
     ]
    }
   ],
   "source": [
    "print(movie_reviews.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rev=(movie_reviews.fileids(\"pos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rev=(movie_reviews.fileids(\"neg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev=nltk.corpus.movie_reviews.words(\"pos/cv025_3108.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hilarious', ',', 'ultra', '-', 'low', 'budget', ...]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_list=[]\n",
    "for rev in neg_rev:\n",
    "    rev_text_neg=rev=nltk.corpus.movie_reviews.words(rev)\n",
    "    review_one_string=\" \".join(rev_text_neg)\n",
    "    review_one_string=review_one_string.replace(' ,',',')\n",
    "    review_one_string=review_one_string.replace(' .','.')\n",
    "    review_one_string=review_one_string.replace(\"\\'\",\"'\")\n",
    "    review_one_string=review_one_string.replace(\" \\'\",\"'\")\n",
    "    rev_list.append(review_one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "for rev in pos_rev:\n",
    "    rev_text_neg=rev=nltk.corpus.movie_reviews.words(rev)\n",
    "    review_one_string=\" \".join(rev_text_neg)\n",
    "    review_one_string=review_one_string.replace(' ,',',')\n",
    "    review_one_string=review_one_string.replace(' .','.')\n",
    "    review_one_string=review_one_string.replace(\"\\'\",\"'\")\n",
    "    review_one_string=review_one_string.replace(\" \\'\",\"'\")\n",
    "    rev_list.append(review_one_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_targets=np.ones((1000,),dtype=np.int)\n",
    "neg_targets=np.zeros((1000,),dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list=[]\n",
    "for neg_tar in neg_targets:\n",
    "    target_list.append(neg_tar)\n",
    "for pos_tar in pos_targets:\n",
    "    target_list.append(pos_tar)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.Series(target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect=CountVectorizer(lowercase=True,stop_words=\"english\",min_df=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_count_vect=count_vect.fit_transform(rev_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 23784)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '007',\n",
       " '05',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '100m',\n",
       " '101',\n",
       " '102',\n",
       " '103',\n",
       " '105',\n",
       " '106',\n",
       " '107',\n",
       " '108',\n",
       " '10th',\n",
       " '11',\n",
       " '110',\n",
       " '113',\n",
       " '115',\n",
       " '11th',\n",
       " '12',\n",
       " '126',\n",
       " '129',\n",
       " '13',\n",
       " '130',\n",
       " '132',\n",
       " '137',\n",
       " '13th',\n",
       " '14',\n",
       " '14th',\n",
       " '15',\n",
       " '150',\n",
       " '1500s',\n",
       " '155',\n",
       " '15th',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '161',\n",
       " '16mm',\n",
       " '16th',\n",
       " '16x9',\n",
       " '17',\n",
       " '175',\n",
       " '1773',\n",
       " '17th',\n",
       " '18',\n",
       " '180',\n",
       " '1800s',\n",
       " '1839',\n",
       " '1869',\n",
       " '1871',\n",
       " '1888',\n",
       " '18th',\n",
       " '19',\n",
       " '1900',\n",
       " '1912',\n",
       " '1914',\n",
       " '1919',\n",
       " '1925',\n",
       " '1928',\n",
       " '1930',\n",
       " '1930s',\n",
       " '1932',\n",
       " '1933',\n",
       " '1935',\n",
       " '1937',\n",
       " '1938',\n",
       " '1939',\n",
       " '1940',\n",
       " '1940s',\n",
       " '1941',\n",
       " '1943',\n",
       " '1944',\n",
       " '1945',\n",
       " '1947',\n",
       " '1948',\n",
       " '1949',\n",
       " '1950',\n",
       " '1950s',\n",
       " '1953',\n",
       " '1954',\n",
       " '1957',\n",
       " '1958',\n",
       " '1959',\n",
       " '1960',\n",
       " '1960s',\n",
       " '1961',\n",
       " '1962',\n",
       " '1963',\n",
       " '1964',\n",
       " '1965',\n",
       " '1966',\n",
       " '1967',\n",
       " '1968',\n",
       " '1969',\n",
       " '1970',\n",
       " '1970s',\n",
       " '1971',\n",
       " '1972',\n",
       " '1973',\n",
       " '1974',\n",
       " '1975',\n",
       " '1976',\n",
       " '1977',\n",
       " '1978',\n",
       " '1979',\n",
       " '1980',\n",
       " '1980s',\n",
       " '1981',\n",
       " '1982',\n",
       " '1983',\n",
       " '1984',\n",
       " '1985',\n",
       " '1986',\n",
       " '1987',\n",
       " '1988',\n",
       " '1989',\n",
       " '1990',\n",
       " '1990s',\n",
       " '1991',\n",
       " '1992',\n",
       " '1993',\n",
       " '1994',\n",
       " '1995',\n",
       " '1996',\n",
       " '1997',\n",
       " '1998',\n",
       " '1998s',\n",
       " '1999',\n",
       " '19th',\n",
       " '1st',\n",
       " '20',\n",
       " '200',\n",
       " '2000',\n",
       " '2001',\n",
       " '2013',\n",
       " '2017',\n",
       " '2020',\n",
       " '2029',\n",
       " '2050',\n",
       " '2058',\n",
       " '20s',\n",
       " '20th',\n",
       " '21',\n",
       " '2176',\n",
       " '21st',\n",
       " '22',\n",
       " '23',\n",
       " '24',\n",
       " '2400',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '25th',\n",
       " '26',\n",
       " '26th',\n",
       " '27',\n",
       " '28',\n",
       " '29',\n",
       " '2d',\n",
       " '2nd',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '30th',\n",
       " '31',\n",
       " '32',\n",
       " '33',\n",
       " '35',\n",
       " '35mm',\n",
       " '36',\n",
       " '360',\n",
       " '37',\n",
       " '39',\n",
       " '3d',\n",
       " '3po',\n",
       " '3rd',\n",
       " '40',\n",
       " '400',\n",
       " '40s',\n",
       " '41',\n",
       " '42',\n",
       " '43',\n",
       " '44',\n",
       " '45',\n",
       " '47',\n",
       " '48',\n",
       " '48th',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '50s',\n",
       " '51',\n",
       " '52',\n",
       " '53',\n",
       " '54',\n",
       " '55',\n",
       " '56',\n",
       " '57',\n",
       " '571',\n",
       " '58',\n",
       " '5th',\n",
       " '60',\n",
       " '600',\n",
       " '6000',\n",
       " '607',\n",
       " '60s',\n",
       " '61',\n",
       " '63',\n",
       " '64',\n",
       " '65',\n",
       " '66',\n",
       " '666',\n",
       " '69',\n",
       " '6th',\n",
       " '70',\n",
       " '700',\n",
       " '70mm',\n",
       " '70s',\n",
       " '73',\n",
       " '75',\n",
       " '76',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '7th',\n",
       " '80',\n",
       " '800',\n",
       " '80s',\n",
       " '81',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '85',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '89',\n",
       " '8mm',\n",
       " '8th',\n",
       " '90',\n",
       " '90210',\n",
       " '90s',\n",
       " '91',\n",
       " '911',\n",
       " '92',\n",
       " '93',\n",
       " '94',\n",
       " '95',\n",
       " '96',\n",
       " '97',\n",
       " '98',\n",
       " '99',\n",
       " '999',\n",
       " '9mm',\n",
       " '_____',\n",
       " '______',\n",
       " '_and_',\n",
       " '_babe_',\n",
       " '_blade',\n",
       " '_brazil_',\n",
       " '_do_',\n",
       " '_does_',\n",
       " '_don',\n",
       " '_film',\n",
       " '_in',\n",
       " '_is_',\n",
       " '_last_',\n",
       " '_life',\n",
       " '_must_',\n",
       " '_not_',\n",
       " '_real_',\n",
       " '_really_',\n",
       " '_saturday_night_live_',\n",
       " '_saving',\n",
       " '_scream_',\n",
       " '_that_',\n",
       " '_the',\n",
       " '_the_',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aaliyah',\n",
       " 'aardman',\n",
       " 'aaron',\n",
       " 'aback',\n",
       " 'abandon',\n",
       " 'abandoned',\n",
       " 'abandoning',\n",
       " 'abandonment',\n",
       " 'abandons',\n",
       " 'abby',\n",
       " 'abc',\n",
       " 'abdomen',\n",
       " 'abducted',\n",
       " 'abduction',\n",
       " 'abductions',\n",
       " 'abe',\n",
       " 'abel',\n",
       " 'aberdeen',\n",
       " 'aberration',\n",
       " 'abetted',\n",
       " 'abetting',\n",
       " 'abhorrent',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'abject',\n",
       " 'ablaze',\n",
       " 'able',\n",
       " 'ably',\n",
       " 'abo',\n",
       " 'aboard',\n",
       " 'abode',\n",
       " 'abolish',\n",
       " 'abolitionists',\n",
       " 'abominable',\n",
       " 'abomination',\n",
       " 'aborted',\n",
       " 'abortion',\n",
       " 'abortions',\n",
       " 'abound',\n",
       " 'abounding',\n",
       " 'abounds',\n",
       " 'abraham',\n",
       " 'abrahams',\n",
       " 'abrams',\n",
       " 'abrasive',\n",
       " 'abroad',\n",
       " 'abrupt',\n",
       " 'abruptly',\n",
       " 'abs',\n",
       " 'absence',\n",
       " 'absences',\n",
       " 'absent',\n",
       " 'absentee',\n",
       " 'absinthe',\n",
       " 'absolute',\n",
       " 'absolutely',\n",
       " 'absolution',\n",
       " 'absorb',\n",
       " 'absorbed',\n",
       " 'absorbing',\n",
       " 'absorbs',\n",
       " 'absorption',\n",
       " 'abstinence',\n",
       " 'abstract',\n",
       " 'absurd',\n",
       " 'absurdist',\n",
       " 'absurdities',\n",
       " 'absurdity',\n",
       " 'absurdly',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'abundantly',\n",
       " 'abuse',\n",
       " 'abused',\n",
       " 'abuser',\n",
       " 'abusers',\n",
       " 'abuses',\n",
       " 'abusing',\n",
       " 'abusive',\n",
       " 'abuzz',\n",
       " 'abysmal',\n",
       " 'abysmally',\n",
       " 'abyss',\n",
       " 'academia',\n",
       " 'academic',\n",
       " 'academy',\n",
       " 'accent',\n",
       " 'accented',\n",
       " 'accents',\n",
       " 'accentuate',\n",
       " 'accentuates',\n",
       " 'accept',\n",
       " 'acceptable',\n",
       " 'acceptance',\n",
       " 'accepted',\n",
       " 'accepting',\n",
       " 'accepts',\n",
       " 'access',\n",
       " 'accessibility',\n",
       " 'accessible',\n",
       " 'accident',\n",
       " 'accidental',\n",
       " 'accidentally',\n",
       " 'accidently',\n",
       " 'accidents',\n",
       " 'acclaim',\n",
       " 'acclaimed',\n",
       " 'accolades',\n",
       " 'accommodates',\n",
       " 'accommodating',\n",
       " 'accompanied',\n",
       " 'accompanies',\n",
       " 'accompaniment',\n",
       " 'accompany',\n",
       " 'accompanying',\n",
       " 'accomplice',\n",
       " 'accomplices',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'accordance',\n",
       " 'according',\n",
       " 'accordingly',\n",
       " 'accordion',\n",
       " 'accosted',\n",
       " 'account',\n",
       " 'accountant',\n",
       " 'accounted',\n",
       " 'accounts',\n",
       " 'accumulated',\n",
       " 'accumulation',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'accurately',\n",
       " 'accusation',\n",
       " 'accusations',\n",
       " 'accuse',\n",
       " 'accused',\n",
       " 'accuses',\n",
       " 'accusing',\n",
       " 'accustomed',\n",
       " 'ace',\n",
       " 'acerbic',\n",
       " 'aces',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achiever',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'achilles',\n",
       " 'achingly',\n",
       " 'acid',\n",
       " 'acidic',\n",
       " 'ack',\n",
       " 'acknowledge',\n",
       " 'acknowledged',\n",
       " 'acknowledges',\n",
       " 'acknowledging',\n",
       " 'acknowledgment',\n",
       " 'acme',\n",
       " 'acquaintance',\n",
       " 'acquaintances',\n",
       " 'acquire',\n",
       " 'acquired',\n",
       " 'acquit',\n",
       " 'acquits',\n",
       " 'acquittal',\n",
       " 'acquitted',\n",
       " 'acres',\n",
       " 'acrimonious',\n",
       " 'acrobatic',\n",
       " 'acrobatics',\n",
       " 'act',\n",
       " 'acted',\n",
       " 'acting',\n",
       " 'action',\n",
       " 'actioner',\n",
       " 'actioners',\n",
       " 'actionfest',\n",
       " 'actions',\n",
       " 'activated',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activist',\n",
       " 'activists',\n",
       " 'activities',\n",
       " 'activity',\n",
       " 'actor',\n",
       " 'actors',\n",
       " 'actress',\n",
       " 'actresses',\n",
       " 'acts',\n",
       " 'actual',\n",
       " 'actuality',\n",
       " 'actualization',\n",
       " 'actualizing',\n",
       " 'actually',\n",
       " 'acumen',\n",
       " 'acupuncture',\n",
       " 'acute',\n",
       " 'acutely',\n",
       " 'ad',\n",
       " 'ad2am',\n",
       " 'adage',\n",
       " 'adam',\n",
       " 'adams',\n",
       " 'adapt',\n",
       " 'adaptation',\n",
       " 'adaptations',\n",
       " 'adapted',\n",
       " 'adapting',\n",
       " 'adaption',\n",
       " 'add',\n",
       " 'addams',\n",
       " 'added',\n",
       " 'addict',\n",
       " 'addicted',\n",
       " 'addiction',\n",
       " 'addictive',\n",
       " 'addicts',\n",
       " 'adding',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'additionally',\n",
       " 'additions',\n",
       " 'addled',\n",
       " 'address',\n",
       " 'addressed',\n",
       " 'addresses',\n",
       " 'addressing',\n",
       " 'adds',\n",
       " 'addy',\n",
       " 'ade',\n",
       " 'adefarasin',\n",
       " 'adept',\n",
       " 'adeptly',\n",
       " 'adequate',\n",
       " 'adequately',\n",
       " 'adhere',\n",
       " 'adherence',\n",
       " 'adheres',\n",
       " 'adjacent',\n",
       " 'adjani',\n",
       " 'adjective',\n",
       " 'adjectives',\n",
       " 'adjoining',\n",
       " 'adjust',\n",
       " 'adjuster',\n",
       " 'adjusting',\n",
       " 'administration',\n",
       " 'admirable',\n",
       " 'admirably',\n",
       " 'admiration',\n",
       " 'admire',\n",
       " 'admired',\n",
       " 'admirer',\n",
       " 'admirers',\n",
       " 'admires',\n",
       " 'admiring',\n",
       " 'admission',\n",
       " 'admit',\n",
       " 'admits',\n",
       " 'admittance',\n",
       " 'admitted',\n",
       " 'admittedly',\n",
       " 'admitting',\n",
       " 'admittingly',\n",
       " 'admonition',\n",
       " 'ado',\n",
       " 'adolescence',\n",
       " 'adolescent',\n",
       " 'adolescents',\n",
       " 'adopt',\n",
       " 'adopted',\n",
       " 'adopting',\n",
       " 'adoption',\n",
       " 'adoptive',\n",
       " 'adopts',\n",
       " 'adorable',\n",
       " 'adoration',\n",
       " 'adore',\n",
       " 'adored',\n",
       " 'adores',\n",
       " 'adorned',\n",
       " 'adrenalin',\n",
       " 'adrenaline',\n",
       " 'adrian',\n",
       " 'adrien',\n",
       " 'adrift',\n",
       " 'adroitly',\n",
       " 'ads',\n",
       " 'adult',\n",
       " 'adulterous',\n",
       " 'adultery',\n",
       " 'adulthood',\n",
       " 'adults',\n",
       " 'advance',\n",
       " 'advanced',\n",
       " 'advancement',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantages',\n",
       " 'advent',\n",
       " 'adventure',\n",
       " 'adventurer',\n",
       " 'adventures',\n",
       " 'adventurous',\n",
       " 'adversarial',\n",
       " 'adversary',\n",
       " 'adverse',\n",
       " 'adversity',\n",
       " 'advertise',\n",
       " 'advertised',\n",
       " 'advertisement',\n",
       " 'advertisements',\n",
       " 'advertiser',\n",
       " 'advertising',\n",
       " 'advice',\n",
       " 'advisable',\n",
       " 'advise',\n",
       " 'advised',\n",
       " 'adviser',\n",
       " 'advisers',\n",
       " 'advises',\n",
       " 'advising',\n",
       " 'advisor',\n",
       " 'advisors',\n",
       " 'advocate',\n",
       " 'advocating',\n",
       " 'aerial',\n",
       " 'aerosmith',\n",
       " 'aesthetic',\n",
       " 'aesthetically',\n",
       " 'aesthetics',\n",
       " 'afar',\n",
       " 'affability',\n",
       " 'affable',\n",
       " 'affair',\n",
       " 'affairs',\n",
       " 'affect',\n",
       " 'affectations',\n",
       " 'affected',\n",
       " 'affecting',\n",
       " 'affection',\n",
       " 'affectionate',\n",
       " 'affectionately',\n",
       " 'affections',\n",
       " 'affects',\n",
       " 'affiliate',\n",
       " 'affiliated',\n",
       " 'affinity',\n",
       " 'affirmation',\n",
       " 'affirmative',\n",
       " 'affirming',\n",
       " 'affleck',\n",
       " 'afflicted',\n",
       " 'affliction',\n",
       " 'affluent',\n",
       " 'afford',\n",
       " 'affordable',\n",
       " 'afforded',\n",
       " 'affraid',\n",
       " 'affront',\n",
       " 'afi',\n",
       " 'aficionado',\n",
       " 'aficionados',\n",
       " 'afield',\n",
       " 'afloat',\n",
       " 'afo',\n",
       " 'afoot',\n",
       " 'afore',\n",
       " 'aforementioned',\n",
       " 'aformentioned',\n",
       " 'afoul',\n",
       " 'afraid',\n",
       " 'africa',\n",
       " 'african',\n",
       " 'africans',\n",
       " 'afro',\n",
       " 'aftereffects',\n",
       " 'afterglow',\n",
       " 'afterlife',\n",
       " 'aftermath',\n",
       " 'afternoon',\n",
       " 'afternoons',\n",
       " 'aftertaste',\n",
       " 'afterthought',\n",
       " 'afterward',\n",
       " 'agape',\n",
       " 'agatha',\n",
       " 'age',\n",
       " 'aged',\n",
       " 'ageing',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agenda',\n",
       " 'agent',\n",
       " 'agents',\n",
       " 'ages',\n",
       " 'aggravating',\n",
       " 'aggression',\n",
       " 'aggressive',\n",
       " 'aggressively',\n",
       " 'agile',\n",
       " 'agility',\n",
       " 'aging',\n",
       " 'agitated',\n",
       " 'ago',\n",
       " 'agonizing',\n",
       " 'agonizingly',\n",
       " 'agony',\n",
       " 'agree',\n",
       " 'agreeable',\n",
       " 'agreed',\n",
       " 'agreeing',\n",
       " 'agreement',\n",
       " 'agrees',\n",
       " 'ah',\n",
       " 'ahabs',\n",
       " 'ahead',\n",
       " 'ahem',\n",
       " 'ahern',\n",
       " 'ahh',\n",
       " 'ahmed',\n",
       " 'aid',\n",
       " 'aidan',\n",
       " 'aide',\n",
       " 'aided',\n",
       " 'aiding',\n",
       " 'aids',\n",
       " 'aiello',\n",
       " 'ailing',\n",
       " 'aim',\n",
       " 'aimed',\n",
       " 'aimee',\n",
       " 'aiming',\n",
       " 'aimless',\n",
       " 'aimlessly',\n",
       " 'aims',\n",
       " 'ain',\n",
       " 'air',\n",
       " 'aircraft',\n",
       " 'aired',\n",
       " 'aires',\n",
       " 'airline',\n",
       " 'airliner',\n",
       " 'airlock',\n",
       " 'airplane',\n",
       " 'airplanes',\n",
       " 'airport',\n",
       " 'airwaves',\n",
       " 'airwolf',\n",
       " 'airy',\n",
       " 'aisle',\n",
       " 'aisles',\n",
       " 'aka',\n",
       " 'aki',\n",
       " 'akin',\n",
       " 'akira',\n",
       " 'akiva',\n",
       " 'akroyd',\n",
       " 'al',\n",
       " 'ala',\n",
       " 'aladdin',\n",
       " 'alain',\n",
       " 'alan',\n",
       " 'alanis',\n",
       " 'alarm',\n",
       " 'alarmed',\n",
       " 'alarming',\n",
       " 'alas',\n",
       " 'alaska',\n",
       " 'albania',\n",
       " 'albanian',\n",
       " 'albeit',\n",
       " 'albert',\n",
       " 'alberta',\n",
       " 'albertson',\n",
       " 'albino',\n",
       " 'albinos',\n",
       " 'album',\n",
       " 'albums',\n",
       " 'alcohol',\n",
       " 'alcoholic',\n",
       " 'alcoholics',\n",
       " 'alcoholism',\n",
       " 'alcott',\n",
       " 'alda',\n",
       " 'aldys',\n",
       " 'alec',\n",
       " 'alejandro',\n",
       " 'alek',\n",
       " 'alert',\n",
       " 'alerted',\n",
       " 'alessandro',\n",
       " 'alex',\n",
       " 'alexander',\n",
       " 'alexandra',\n",
       " 'alexandre',\n",
       " 'alfonso',\n",
       " 'alfre',\n",
       " 'alfred',\n",
       " 'algar',\n",
       " 'ali',\n",
       " 'alias',\n",
       " 'aliases',\n",
       " 'alice',\n",
       " 'alicia',\n",
       " 'alida',\n",
       " 'alien',\n",
       " 'alienate',\n",
       " 'alienated',\n",
       " 'alienates',\n",
       " 'alienating',\n",
       " 'alienation',\n",
       " 'aliens',\n",
       " 'align',\n",
       " 'alignment',\n",
       " 'alike',\n",
       " 'alison',\n",
       " 'alive',\n",
       " 'allah',\n",
       " 'allan',\n",
       " 'allegations',\n",
       " 'alleged',\n",
       " 'allegedly',\n",
       " 'allegiance',\n",
       " 'allegiances',\n",
       " 'allegory',\n",
       " 'allegra',\n",
       " 'allen',\n",
       " 'allergy',\n",
       " 'alleviate',\n",
       " 'alley',\n",
       " 'alleys',\n",
       " 'alliance',\n",
       " 'alliances',\n",
       " 'allied',\n",
       " 'allies',\n",
       " 'alligator',\n",
       " 'alligators',\n",
       " 'allison',\n",
       " 'allow',\n",
       " 'allowed',\n",
       " 'allowing',\n",
       " 'allows',\n",
       " 'alluded',\n",
       " 'allure',\n",
       " 'alluring',\n",
       " 'allusion',\n",
       " 'allusions',\n",
       " 'ally',\n",
       " 'alma',\n",
       " 'almod',\n",
       " 'alongside',\n",
       " 'aloof',\n",
       " 'alot',\n",
       " 'aloud',\n",
       " 'alright',\n",
       " 'altar',\n",
       " 'alter',\n",
       " 'alteration',\n",
       " 'alterations',\n",
       " 'altered',\n",
       " 'altering',\n",
       " 'alternate',\n",
       " 'alternately',\n",
       " 'alternates',\n",
       " 'alternating',\n",
       " 'alternative',\n",
       " 'alternatively',\n",
       " 'alters',\n",
       " 'althea',\n",
       " 'altman',\n",
       " 'altogether',\n",
       " 'altough',\n",
       " 'alum',\n",
       " 'alumni',\n",
       " 'alumnus',\n",
       " 'alvarado',\n",
       " 'alvin',\n",
       " 'alyson',\n",
       " 'alyssa',\n",
       " 'alzheimer',\n",
       " 'amadeus',\n",
       " 'amalgamation',\n",
       " 'amanda',\n",
       " 'amarillo',\n",
       " 'amateur',\n",
       " 'amateurish',\n",
       " 'amateurishly',\n",
       " 'amaze',\n",
       " 'amazed',\n",
       " 'amazement',\n",
       " 'amazes',\n",
       " 'amazing',\n",
       " 'amazingly',\n",
       " 'amazon',\n",
       " 'ambassador',\n",
       " 'ambassadors',\n",
       " 'amber',\n",
       " 'ambiance',\n",
       " 'ambience',\n",
       " 'ambient',\n",
       " 'ambiguities',\n",
       " 'ambiguity',\n",
       " 'ambiguous',\n",
       " 'ambiguously',\n",
       " 'ambition',\n",
       " 'ambitions',\n",
       " 'ambitious',\n",
       " 'ambitiously',\n",
       " 'ambivalence',\n",
       " 'ambivalent',\n",
       " 'ambrose',\n",
       " 'ambulance',\n",
       " 'ambush',\n",
       " 'amc',\n",
       " 'amen',\n",
       " 'amendment',\n",
       " 'amends',\n",
       " 'america',\n",
       " 'american',\n",
       " 'americanized',\n",
       " 'americans',\n",
       " 'americas',\n",
       " 'ames',\n",
       " 'amiable',\n",
       " 'amicable',\n",
       " 'amid',\n",
       " 'amidala',\n",
       " 'amidst',\n",
       " 'amis',\n",
       " 'amish',\n",
       " 'amiss',\n",
       " 'amistad',\n",
       " 'ammo',\n",
       " 'ammunition',\n",
       " 'amnesia',\n",
       " 'amnesiac',\n",
       " 'amok',\n",
       " 'amon',\n",
       " 'amoral',\n",
       " 'amorality',\n",
       " 'amos',\n",
       " 'amounted',\n",
       " 'amounts',\n",
       " 'amphetamines',\n",
       " 'amphibian',\n",
       " 'ample',\n",
       " 'amplified',\n",
       " 'amply',\n",
       " 'amputated',\n",
       " 'amuck',\n",
       " 'amuse',\n",
       " 'amused',\n",
       " 'amusement',\n",
       " 'amusements',\n",
       " 'amusing',\n",
       " 'amusingly',\n",
       " 'amy',\n",
       " 'anaconda',\n",
       " 'anacondas',\n",
       " 'anakin',\n",
       " 'anal',\n",
       " 'analogy',\n",
       " 'analyses',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analysts',\n",
       " 'analyze',\n",
       " 'analyzed',\n",
       " 'analyzing',\n",
       " 'anand',\n",
       " 'anarchic',\n",
       " 'anarchists',\n",
       " 'anarchy',\n",
       " 'anastasia',\n",
       " 'anatomy',\n",
       " 'ancestors',\n",
       " 'anchor',\n",
       " 'anchors',\n",
       " 'ancient',\n",
       " 'anders',\n",
       " 'anderson',\n",
       " 'andersons',\n",
       " 'andie',\n",
       " 'andre',\n",
       " 'andrea',\n",
       " 'andreas',\n",
       " 'andrew',\n",
       " 'andrews',\n",
       " 'android',\n",
       " 'androids',\n",
       " 'andromeda',\n",
       " 'andrzej',\n",
       " 'andy',\n",
       " 'anecdotal',\n",
       " 'anecdote',\n",
       " 'anecdotes',\n",
       " 'anemic',\n",
       " 'anew',\n",
       " 'ang',\n",
       " 'angel',\n",
       " 'angela',\n",
       " 'angeles',\n",
       " 'angelic',\n",
       " 'angelina',\n",
       " 'angelo',\n",
       " 'angels',\n",
       " 'anger',\n",
       " ...]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_names=count_vect.get_feature_names()\n",
    "x_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_count_vect=pd.DataFrame(x_count_vect.toarray(),columns=x_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x_count_vect,y,test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GaussianNB()\n",
    "y_pred=model.fit(x_train,y_train).predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6675"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8225"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8225\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.accuracy_score(y_predict,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[181  30]\n",
      " [ 41 148]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
